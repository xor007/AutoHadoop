#!/usr/bin/env bash
# hadoop_cluster
# setup a low memory but complete hadoop cluster that can run within a 
# restricted 4GB RAM laptop running Ubuntu 16.04
# references:
# - docker manual:https://docs.docker.com/engine/installation/linux/ubuntulinux/
# - hadoop cluster MOOC:https://www.udacity.com/course/deploying-a-hadoop-cluster--ud1000
# this cluster should be enough to complete the hadoop cluster MOOC

docker_key="58118E89F3A912897C070ADBF76221572C52609D"
docker_repo="https://apt.dockerproject.org/repo"
distro="ubuntu-wily main"
thin_img="phusion/baseimage"
ip_prefix="192.168.7"

jdk_update=141
jdk_v=8
jdk_version="$jdk_v"u"$jdk_update"
jdk_dir=jdk1."$jdk_v".0_"$jdk_update"
jdk_file=jdk-$jdk_version-linux-x64.tar.gz
jdk_link=http://download.oracle.com/otn-pub/java/jdk/$jdk_version-b15/336fa29ff2bb4ef291e347e091f7f4a7/$jdk_file

hadoop_version=2.7.3
hadoop_file=hadoop-$hadoop_version.tar.gz
hadoop_link=https://archive.apache.org/dist/hadoop/core/hadoop-$hadoop_version/$hadoop_file

spark_version=2.1.0
spark_file=spark-$spark_version-bin-without-hadoop.tgz
spark_link=https://archive.apache.org/dist/spark/spark-$spark_version/$spark_file


get_string()
{
	name=$1
	default=$2
	echo "Enter $name, then press enter [$default]:" >&2
	read value
	if [[ "$value" == "" ]]; then
		value=$default
	fi
	echo $value
	
}


if [ -x ~/.hadoop_cluster ]; then
  source ~/.hadoop_cluster
else
  ip_prefix="192.168.7"
  master_name="mr"
  slave_name="slv"
  user_name="student"
fi

get_env(){
  cat <<EOF
-----------------------------------------------------
                  Cluster Details
-----------------------------------------------------
    Master  : $master_name/$ip_prefix.253
    Slaves  : $slave_name[1-3]/$ip_prefix.1[1-3]
    User    : $user_name
EOF
}


set_env(){
  ip_prefix=$(get_string "ip_prefix" "$ip_prefix")
  master_name=$(get_string "master_prefix" "$master_name")
  slave_name=$(get_string "slave_prefix" "$slave_name")
  user_name=$(get_string "user_name" "$user_name")

  cat <<EOF >~/.hadoop_cluster
ip_prefix=$ip_prefix
master_name=$master_name
slave_name=$slave_name
user_name=$user_name
EOF

  chmod +x ~/.hadoop_cluster
}


check_internet()
{
	wget -O - http://github.com | grep html && echo 'Internet connection detected!' || ( echo 'NO Internet connection! Exiting...'; kill 0 )
}

setup_docker()
{

	echo 'Removing default ubuntu docker version...'
	sudo apt-get purge lxc-docker

	echo 'Setting up docker repository...'
	sudo apt-get update
	sudo apt-get install apt-transport-https ca-certificates
	sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys $docker_key
	sudo cat <<EOF >/tmp/docker.list
deb $docker_repo $distro
EOF
  sudo cp -v /tmp/docker.list /etc/apt/sources.list.d/
	sudo apt-get update
	( apt-cache policy docker-engine|grep http |grep -v docker ) && ( echo 'old repo for docker found, manually clean and try again. Exiting...'; kill 0 )

	echo 'adding recommended packages...'
	sudo apt-get install linux-image-extra-$(uname -r)

	echo 'installing docker...'
	sudo apt-get install docker-engine -y

	echo 'Testing docker...'
	sudo service docker start
	sudo systemctl enable docker
	echo 'Will now attempt to install and run a basic docker image. Hold ctrl and press `d` to exit.'
	sudo docker run hello-world
}
deploy_hadoop()
{
	echo 'Creating a docker based virtual network for the hadoop cluster...'
	sudo docker network create --subnet=$ip_prefix.0/24 hadoop-net

	
	echo "Creating master hadoop node from $thin_img image..."
	sudo docker create --name $master_name \
	--net hadoop-net \
	--ip $ip_prefix.253 \
	--hostname $master_name \
	--add-host $slave_name"1":$ip_prefix.11 \
	--add-host $slave_name"2":$ip_prefix.12 \
	--add-host $slave_name"3":$ip_prefix.13 \
	$thin_img
	sudo docker start $master_name

	echo 'Please enter root password for your cluster nodes [2016w1+Z]'
	read pass
	if [ "$pass" == "" ]
	then
		pass="2016w1+Z"
	fi
	echo -e "$pass\n$pass" | sudo docker exec -i $master_name passwd
	sudo docker exec -i $master_name groupadd hadoop
	sudo docker exec -i $master_name useradd -m -s /bin/bash -g hadoop $user_name
	echo -e "$pass\n$pass" | sudo docker exec -i $master_name passwd $user_name

	echo "installing java jdk and hadoop in master node as user $user_name..."
	(sudo docker exec -i $master_name apt-get update) && (sudo docker exec -i $master_name apt-get -y install wget) && (sudo docker exec -i $master_name apt-get -y install python)
  echo 'press enter to continue...'; read ta
	(sudo docker exec -i $master_name wget --no-cookies --no-check-certificate --header 'Cookie: oraclelicense=accept-securebackup-cookie' $jdk_link -O $jdk_file) && (	sudo docker exec -i $master_name tar -zxvpf $jdk_file -C /usr/local) && (sudo docker exec -i $master_name  ln -s /usr/local/$jdk_dir /usr/local/java ) && ( sudo docker exec -i $master_name rm $jdk_file)
  echo 'press enter to continue...'; read ta
	( sudo docker exec -i $master_name wget $hadoop_link -O $hadoop_file) && ( 	sudo docker exec -i $master_name  tar -zxvpf $hadoop_file -C /usr/local ) && (sudo docker exec -i $master_name  ln -s /usr/local/hadoop-$hadoop_version /usr/local/hadoop ) && ( 	sudo docker exec -i $master_name  rm $hadoop_file )
  echo 'press enter to continue...'; read ta
	( sudo docker exec -i $master_name chown -R $user_name:hadoop /usr/local/hadoop-$hadoop_version) && ( 	sudo docker exec -i $master_name chmod -R ug+rw /usr/local/hadoop-$hadoop_version )
  echo 'press enter to continue...'; read ta

	( sudo docker exec -u $user_name -i $master_name sh -c "echo 'export JAVA_HOME=/usr/local/java' >>/home/$user_name/.bashrc" ) && ( sudo docker exec -i $master_name sh -c "echo 'export PATH=\$JAVA_HOME/bin:\$PATH' >>/home/$user_name/.bashrc" ) && ( sudo docker exec -i $master_name sh -c "echo 'export HADOOP_HOME=/usr/local/hadoop' >>/home/$user_name/.bashrc" )
  echo 'press enter to continue...'; read ta
	( sudo docker exec -u $user_name -i $master_name sh -c "echo 'export PATH=\$HADOOP_HOME/bin:\$PATH' >>/home/$user_name/.bashrc" ) && ( sudo docker exec -i $master_name sh -c "echo 'export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop' >>/home/$user_name/.bashrc" )
( sudo docker exec -i $master_name sh -c "echo 'export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true' >>/home/$user_name/.bashrc" )
( sudo docker exec -i $master_name sh -c "echo 'export HADOOP_CLASSPATH=\${JAVA_HOME}/lib/tools.jar' >>/home/$user_name/.bashrc" )

( sudo docker exec -i $master_name sh -c "echo 'export LD_LIBRARY_PATH=\${HADOOP_HOME}/lib/native/:\${LD_LIBRARY_PATH}' >>/home/$user_name/.bashrc" )

}

enable_ssh_server_in_node()
{
	echo "enabling ssh server in node..."
  echo 'press enter to continue...'; read ta
  sudo docker exec -i $master_name /etc/my_init.d/00_regen_ssh_host_keys.sh
  sudo docker exec -i $master_name rm -f /etc/service/sshd/down
  sudo docker stop $master_name
  sudo docker start $master_name
}

config_passwordless_ssh()
{
	echo "configuring ssh client in host..."
  echo 'press enter to continue...'; read ta
  cat <<EOF | sed -e"s/slv/$slave_name/;" >>~/.ssh/config

## Hadoop cluster nodes
Host $master_name
  Hostname $ip_prefix.253
  User $user_name
Host slv1
  Hostname $ip_prefix.11
  User $user_name
Host slv2
  Hostname $ip_prefix.12
  User $user_name
Host slv3
  Hostname $ip_prefix.13
  User $user_name

EOF
	echo '###setting up passwordless authentication between nodes...'
	echo 'generating ssh keys in host...'
  echo 'press enter to continue...'; read ta
  ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa
  echo 'press enter to continue...'; read ta
	echo 'generating ssh keys in master node...'
  sudo docker exec -u $user_name -i $master_name sh -c "ssh-keygen -t rsa -N '' -f /home/$user_name/.ssh/id_rsa"
  sudo docker exec -u $user_name -i $master_name sh -c "cat /home/$user_name/.ssh/id_rsa.pub >> /home/$user_name/.ssh/authorized_keys"
  cat ~/.ssh/id_rsa.pub | sudo docker exec -u $user_name -i $master_name sh -c "cat - >> /home/$user_name/.ssh/authorized_keys"
  sudo docker exec -u $user_name -i $master_name chmod 600 /home/$user_name/.ssh/authorized_keys

  echo 'press enter to continue...'; read ta
	echo "configuring ssh client in node..."
cat <<EOF | sed -e"s/slv/$slave_name/;"  | sudo docker exec -u $user_name -i $master_name sh -c "cat - > /home/$user_name/.ssh/config; chmod 600 /home/$user_name/.ssh/config"

## Hadoop cluster nodes
Host $master_name
  Hostname $ip_prefix.253
  User $user_name
Host slv1
  Hostname $ip_prefix.11
  User $user_name
Host slv2
  Hostname $ip_prefix.12
  User $user_name
Host slv3
  Hostname $ip_prefix.13
  User $user_name

EOF

}

setup_hadoop_cluster()
{
  deploy_hadoop
  enable_ssh_server_in_node
  config_passwordless_ssh
	echo 'setting up hadoop node in master mode...'
  echo 'press enter to continue...'; read ta
	#core-site.xml
  sudo docker exec -u $user_name -i $master_name sh -c "sed -i -E -e's~export\ JAVA_HOME.+$~export\ JAVA_HOME=/usr/local/java~;' /usr/local/hadoop/etc/hadoop/hadoop-env.sh"
cat <<EOF | sudo docker exec -u $user_name -i $master_name sh -c "cat - > /usr/local/hadoop/etc/hadoop/core-site.xml"
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://$master_name:9000</value>
  </property>
</configuration>
EOF

	#mapred-site.xml
cat <<EOF | sudo docker exec -u $user_name -i $master_name sh -c "cat - > /usr/local/hadoop/etc/hadoop/mapred-site.xml"
<configuration>
  <property>
    <name>mapreduce.jobtracker.address</name>
    <value>$master_name:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

  <!--property>
    <name>mapreduce.map.memory.mb</name>
    <value>256</value>
  </property>

  <property>
   <name>mapreduce.reduce.memory.mb</name>
   <value>512</value>
  </property>

  <property>
    <name>mapreduce.map.java.opts</name>
    <value>-Xmx192m</value>
  </property>

  <property>
   <name>mapreduce.reduce.java.opts</name>
   <value>-Xmx384m</value>
  </property-->

</configuration>
EOF

	#yarn-site.xml
cat <<EOF | sudo docker exec -u $user_name -i $master_name sh -c "cat - > /usr/local/hadoop/etc/hadoop/yarn-site.xml"
<configuration>

<!-- Site specific YARN configuration properties -->

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>$master_name</value>
  </property>
  <!--property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>1024</value>
  </property>
  <property>
   <name>yarn.scheduler.minimum-allocation-mb</name>
   <value>256</value>
  </property-->
</configuration>
EOF

  echo '### setting up slave nodes...'
  echo 'press enter to continue...'; read ta
  sudo docker commit $master_name hadoop_base
	sudo docker create --name $slave_name"1" --net hadoop-net --ip $ip_prefix.11 --hostname $slave_name"1"	hadoop_base
	sudo docker create --name $slave_name"2" --net hadoop-net --ip $ip_prefix.12 --hostname $slave_name"2"	hadoop_base
	sudo docker create --name $slave_name"3" --net hadoop-net --ip $ip_prefix.13 --hostname $slave_name"3"	hadoop_base

  for num in {1..3}; do
    sudo docker start $slave_name$num
  done

  echo 'setting up hdfs-site config...'
cat <<EOF | sudo docker exec -u $user_name -i $master_name sh -c "cat - > /usr/local/hadoop/etc/hadoop/hdfs-site.xml"
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
  </property>
</configuration>
EOF
  sudo docker exec -u $user_name -i $master_name sh -c "mkdir -p /usr/local/hadoop/data/hdfs/namenode"
  for num in {1..3}; do
    cat <<EOF | sudo docker exec -u $user_name -i $slave_name$num sh -c "cat - > /usr/local/hadoop/etc/hadoop/hdfs-site.xml"
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
  </property>
</configuration>
EOF
  sudo docker exec -u $user_name -i $slave_name$num sh -c "mkdir -p /usr/local/hadoop/data/hdfs/datanode"
done

  echo 'masters and slaves...'
  cat <<EOF | sudo docker exec -u $user_name -i $master_name sh -c "cat - > /usr/local/hadoop/etc/hadoop/masters"
localhost
$master_name
EOF
  cat <<EOF | sed -e"s/slv/$slave_name/;" | sudo docker exec -u $user_name -i $master_name sh -c "cat - > /usr/local/hadoop/etc/hadoop/slaves"
slv1
slv2
slv3
EOF

  echo 'formatting the hadoop file system, press enter to continue...'; read ta
  sudo docker exec -u $user_name -i $master_name bash -c "/usr/local/hadoop/bin/hdfs namenode -format"


  echo 'restricting the RAM usage...'
}

deploy_spark()
{
	(sudo docker exec -i $master_name wget --no-cookies --no-check-certificate $spark_link -O $spark_file) && (	sudo docker exec -i $master_name tar -zxvpf $spark_file -C /usr/local) && ( sudo docker exec -i $master_name rm $spark_file)
  echo 'press enter to continue...'; read ta
	( sudo docker exec -i $master_name chown -R $user_name:hadoop /usr/local/$spark_file) && ( 	sudo docker exec -i $master_name chmod -R ug+rw /usr/local/hadoop-$hadoop_version )
  (sudo docker exec -i $master_name  ln -s /usr/local/$spark_file /usr/local/spark )

  cat <<EOF | sudo docker exec -u $user_name -i $master_name  sh -c "cat - >> /usr/local/spark/conf/spark-env.sh"
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export SPARK_EXECUTOR_INSTANCES=3
export SPARK_DIST_CLASSPATH=$(hadoop classpath)
EOF

  cat <<EOF | sudo docker exec -u $user_name -i $master_name  sh -c "cat - >>~/.bashrc" 
export SPARK_HOME=/usr/local/spark
export PATH=$SPARK_HOME/bin:$PATH
EOF

#/usr/local/spark-2.0.0-bin-without-hadoop
}

check_cluster()
{
	echo 'verifying docker installation...'
	echo 'verifying hadoop cluster network...'
}

start_cluster()
{
  start_docker
  start_hadoop
}

start_docker()
{
  echo 'starting instances...'
  sudo docker start $master_name
  for num in {1..3}; do
    sudo docker start $slave_name$num
  done
}

start_hadoop(){
  echo 'starting master processes...'
  sudo docker exec -u $user_name -i $master_name bash -c "/usr/local/hadoop/sbin/start-dfs.sh"
  sudo docker exec -u $user_name -i $master_name bash -c "/usr/local/hadoop/sbin/start-yarn.sh"
  sudo docker exec -u $user_name -i $master_name bash -c "/usr/local/hadoop/sbin/$master_name-jobhistory-daemon.sh start historyserver"
  sudo docker exec -u $user_name -i $master_name bash -l -c "/usr/local/java/bin/jps"
}

stop_cluster()
{
  stop_hadoop
  stop_docker
}

stop_hadoop()
{
  echo 'stoping processes...'
  sudo docker exec -u $user_name -i $master_name bash -c "/usr/local/hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver"
  sudo docker exec -u $user_name -i $master_name bash -c "/usr/local/hadoop-2.7.2/sbin/stop-all.sh"
}

stop_docker(){
  echo 'stoping instances...'
  sudo docker stop $master_name
  for num in {1..3}; do
    sudo docker stop $slave_name$num
  done
}



destroy_cluster()
{
  sudo docker stop $(sudo docker ps -a -q)
  sudo docker rm $(sudo docker ps -a -q)
  sudo docker network rm hadoop-net
}

get_env

case "$1" in
  start)
    start_cluster
    ;;
  stop)
    stop_cluster
    ;;
  uninstall)
    destroy_cluster
    ;;
  install)
    #check_internet
    #setup_docker
    setup_hadoop_cluster
    ;;
  addspark)
    deploy_spark
    ;;
  startdkr)
    start_docker
    ;;
  stopdkr)
    stop_docker
    ;;
  stophdp)
    stop_hadoop
    ;;
  starthdp)
    start_hadoop
    ;;
  retart)
    stop_cluster
    start_cluster
    ;;
  *)
    echo "Usage: $0 {start|stop|restart|install|uninstall|addspark|stophdp|starthdp|startdkr|stopdkr}"
esac
